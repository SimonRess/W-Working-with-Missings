---
title: |
       <center>
       ![](Slides_files/RUB.jpg){width=2.5in}
       </center>
subtitle:  "Multiple Imputation and subsequent calculations"
author: "Simon Ress | Ruhr-Universität Bochum"
institute: "Workshop at hr&c, Bochum, 2021"
date: "September 22, 2022"

fontsize: 10pt

output:
  beamer_presentation:
    keep_md: true
    keep_tex: no
    latex_engine: xelatex
    #theme: metropolis
    slide_level: 2 # which header level should be printed as slides
    incremental: no
header-includes:
  - \usetheme[numbering=fraction]{metropolis}
#Define footer:
  - \definecolor{beaublue}{RGB}{182, 203, 201} #{0.74, 0.83, 0.9}
  - \setbeamertemplate{frame footer}{\tiny{\textcolor{beaublue}{Workshop Multiple Imputation and subsequent calculations (at hr\&c),  2022 | SIMON RESS}}}
#hide footer on title page:
  - |
    \makeatletter
    \def\ps@titlepage{%
      \setbeamertemplate{footline}{}
    }
    \addtobeamertemplate{title page}{\thispagestyle{titlepage}}{}
    \makeatother
#show footer on section's start/title pages:
  #overwrite "plain,c,noframenumbering" in section pages definition -> enables footer:
  - |
    \makeatletter
    \renewcommand{\metropolis@enablesectionpage}{
      \AtBeginSection{
        \ifbeamer@inframe
          \sectionpage
        \else
          \frame[c]{\sectionpage}
        \fi
      }
    }
    \metropolis@enablesectionpage
    \makeatother
  #define footer of section pages:
  - |
    \makeatletter
    \def\ps@sectionpage{%
      \setbeamertemplate{frame footer}{\tiny{\textcolor{beaublue}{Workshop Multiple Imputation and subsequent calculations (at hr\&c),  2022 | SIMON RESS}}}
    }
    \addtobeamertemplate{section page}{\thispagestyle{sectionpage}}{}
    \makeatother
#add secrtion numbers to TOC:
  - |
    \setbeamertemplate{section in toc}{
    \leavevmode%
    \inserttocsectionnumber. 
    \inserttocsection\par%
    }
    \setbeamertemplate{subsection in toc}{
    \leavevmode\leftskip=2.5em\inserttocsubsection\par     }
#Adjust representation of chunks
  #Reduce space between code chunks and code output
  - |
    \setlength{\OuterFrameSep}{-4pt}
    \makeatletter
    \preto{\@verbatim}{\topsep=-10pt \partopsep=-10pt }
    \makeatother
  #Change background-color of source-code
  - \definecolor{shadecolor}{RGB}{240,240,240}
  #Set a frame around the results
  - | 
    \let\verbatim\undefined
    \let\verbatimend\undefined
    \DefineVerbatimEnvironment{verbatim}{Verbatim}{frame=single, rulecolor=\color{shadecolor}, framerule=0.3mm,framesep=1mm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# if(!require(tinytex)) install.packages("tinytex")
# tinytex::is_tinytex()
# tinytex::reinstall_tinytex()
# tinytex::install_tinytex()
# tintytex::parse_packages()
# tinytex::tl_pkgs()
# tinytex::parse_install("metropolis")

# Markdown text sizes: Huge > huge > LARGE > Large > large > normalsize > small > footnotesize > scriptsize > tiny
#Usage: \tiny

if(!require("dplyr")) install.packages("dplyr")
library(dplyr)

```


## Content
\tableofcontents[]

# Introduction
**What is this workshop about?**

- Specific methods for working with multiple imputed datasets

**Why are special methods for working with multiple imputed data important?**

- We always want to estimate the true (causal) effect of an event

**Does just using a specific algorithms leads to unbiased estimates?**

- No, there are other effects influencing the bias of an estimate

This workshop therefore follows a **holistic approach**. Based on methods for **calculating unbiased effects** in data without missing values, the different **missing patterns** are then introduced, the effect of different **missing-handling techniques** on the bias is demonstrated and then **multiple imputation** is introduced as a method for reducing the bias.



# Examplary Data Set

## Planing the interdependencies of variables

- Every variable is constructed by an *random term*
- Some variables are influenced by *values of other variables*
- **e.g. "Mann" = 1 increases the probability for FK=1 by 20%**

```{r planing-dataset-creation, out.width = "100%", out.height="90%", message=FALSE}
#Link: https://mermaid-js.github.io/mermaid/#/flowchart  # , fig.width = 5,    , fig.asp = 2 fig.dim = c(6,2), 
#Libraries:
  if(!require(DiagrammeR)) install.packages("DiagrammeR")
  library(DiagrammeR)
  #required fpr mermaid usage in pdf-files:
    #if(!require(webshot)) install.packages("webshot")
    #webshot::install_phantomjs()

 DiagrammeR::mermaid("
  graph LR
    Mann-->|+20%|FK
    Alter-->|+10%|FK
    FK-->|+2.0|B2.1
    FK-->|+1.5|B5.1
    B2.1-->|+0.2|B5.1
    Alter-->| -0.30|B5.1
  ", width = "100%", height = "100%") # %>% plotly::save_image(., file = "aa.png")
```



## Data Set Creation

\footnotesize
```{r dataset-creation, echo=TRUE}
#Create variables
set.seed(415)
Mann = ifelse(runif(5000,0,1) < 0.50, 1, 0)
Alter = as.numeric(cut(runif(5000,20,70), 
                       c(20,30,40,50,60,70)))
FK = ifelse((Mann*0.2 + Alter*0.1 + 
             runif(5000,0,0.6)) > 0.95, 1, 0)
B2.1 = as.numeric(cut(FK*2 + 
                      rnorm(5000,2,0.35), c(0,1,2,3,4,6)))
set.seed(1015)
B5.1 = as.numeric(cut(FK*1.5 + B2.1*0.2 + Alter*(-0.30) + 
                      rnorm(5000,2.5,0.30), c(-2,1,2,3,4,8)))

#Build data frame
df = data.frame(Mann, Alter, FK, B2.1, B5.1)
```
\normalsize


## View Data Set
```{r dataset-head, echo=TRUE}
head(df,10)
```


## A. Check whether true effects can be estimated
\scriptsize
```{r I-Check-Calculation-of-Effects, echo=FALSE, message=FALSE}
# Mann -(+0.2)-> FK
  knitr::kable(summary(lm(FK~Mann, data = df))$coefficients, digits = 4, caption = "lm(FK~Mann) | Mann: +0.2", format = 'markdown')
  
# Alter -(+0.1)-> FK
  knitr::kable(summary(lm(FK~Alter, data = df))$coefficients, digits = 4, caption = "lm(FK~Alter) | Alter: +0.1", format = 'markdown')

# FK -(+2)-> B2.1
  knitr::kable(summary(lm(B2.1~FK, data = df))$coefficients, digits = 4, caption = " | FK: +2", format = 'markdown')
```
\normalsize

## A. Evaluation whether true effects was estimated

Yes, in all three models the true effect was estimated (taken the confidence interval into account).


## B. Check whether true effects can be estimated 
\scriptsize
```{r II-Check-Calculation-of-Effects, echo=FALSE, message=FALSE}
# FK -(+1.5)-> B5.1
  knitr::kable(summary(lm(B5.1~FK, data = df))$coefficients, digits = 4, caption = "lm(B5.1~FK) | FK: +1.5")

# B2.1 -(+0.2)-> B5.1
  knitr::kable(summary(lm(B5.1~B2.1, data = df))$coefficients, digits = 4, caption = "lm(B5.1~B2.1) | B2.1: +0.2")

# Alter -(-0.3)-> B5.1
  knitr::kable(summary(lm(B5.1~Alter, data = df))$coefficients, digits = 4, caption = "lm(B5.1~Alter) | Alter: -0.3")
```
\normalsize


## B. Evaluation whether true effects was estimated
- It is not obvious from the estimates and standard error whether the true effect was calculated
- Verification by the exact calculation of the confidence interval is needed


## B.II Check if confidence interval encloses true effect
**Recap Confidence Interval**
A confidence interval is an interval that contains the population parameter with probability $1 − \alpha$. A confidence interval takes on the form:
$$\overline{X}±t_{α/2,N−1}*S_{\overline{X}}$$

where $t_{α/2,N−1}$ is the (z-)value needed to generate an area of $\alpha/2$ in each tail of a t-distribution with n-1 degrees of freedom and  
$S_{\overline{X}} = \frac{s}{\sqrt{N}}$ is the standard error of the mean (s=standard deviation).
<!-- Link to formulars & explaination: https://bookdown.org/logan_kelly/r_practice/p09.html -->
<!-- Mathematics in R Markdown: https://rpruim.github.io/s341/S19/from-class/MathinRmd.html -->
<!-- Writing Mathematic Fomulars in Markdown: https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/ -->

| Z-value (standard deviation) |	p-value (probability) |	Confidence level |
| --- | --- | --- |
| < -1,65 oder > +1,65 | < 0,10 | 90% | 
| < -1,96 oder > +1,96 | < 0,05 | 95% |
| < -2,58 oder > +2,58 | < 0,01 | 99% | 
<!--Z/p-value table: https://pro.arcgis.com/de/pro-app/latest/tool-reference/spatial-statistics/what-is-a-z-score-what-is-a-p-value.htm -->


## B.II Check if confidence interval encloses true effect (B5.1~FK)
\scriptsize
```{r Check-Calculation-of-Effects (B5.1~FK), echo=FALSE, message=FALSE}
# FK -(+1.5)-> B5.1
  knitr::kable(summary(lm(B5.1~FK, data = df))$coefficients, digits = 4, caption = "lm(B5.1~FK) | FK: +1.5")
```
\normalsize

- **True effect: +1.5**
- Confidence Interval = $\overline{X}±t_{α/2,N−1}*S_{\overline{X}}$
- **lower.bound** = `r summary(lm(B5.1~FK, data = df))$coefficients[2,1]` - `r summary(lm(B5.1~FK, data = df))$coefficients[2,2]` * 1.96 = **`r round(summary(lm(B5.1~FK, data = df))$coefficients[2,1] - summary(lm(B5.1~FK, data = df))$coefficients[2,2] * 1.96 ,3)`**
- **upper.bound** = `r summary(lm(B5.1~FK, data = df))$coefficients[2,1]` + `r summary(lm(B5.1~FK, data = df))$coefficients[2,2]` * 1.96 = **`r round(summary(lm(B5.1~FK, data = df))$coefficients[2,1] + summary(lm(B5.1~FK, data = df))$coefficients[2,2] * 1.96 ,3)`**

-> True effect is **not covered** by the confidence interval


## B.II Check if confidence interval encloses true effect (B5.1~B2.1)
\scriptsize
```{r Check-Calculation-of-Effects (B5.1~B2.1), echo=FALSE, message=FALSE}
# B2.1 -(+0.2)-> B5.1
  knitr::kable(summary(lm(B5.1~B2.1, data = df))$coefficients, digits = 4, caption = "lm(B5.1~B2.1) | B2.1: +0.2")
```
\normalsize

- **True effect: +0.2**
- Confidence Interval = $\overline{X}±t_{α/2,N−1}*S_{\overline{X}}$
- **lower.bound** = `r summary(lm(B5.1~B2.1, data = df))$coefficients[2,1]` - `r summary(lm(B5.1~B2.1, data = df))$coefficients[2,2]` * 1.96 = **`r round(summary(lm(B5.1~B2.1, data = df))$coefficients[2,1] - summary(lm(B5.1~B2.1, data = df))$coefficients[2,2] * 1.96 ,3)`**
- **upper.bound** = `r summary(lm(B5.1~B2.1, data = df))$coefficients[2,1]` + `r summary(lm(B5.1~B2.1, data = df))$coefficients[2,2]` * 1.96 = **`r round(summary(lm(B5.1~B2.1, data = df))$coefficients[2,1] + summary(lm(B5.1~B2.1, data = df))$coefficients[2,2] * 1.96 ,3)`**

-> True effect is **not covered** by the confidence interval


## B.II Check if confidence interval encloses true effect (B5.1~Alter)
\scriptsize
```{r Check-Calculation-of-Effects (B5.1~Alter), echo=FALSE, message=FALSE}
# Alter -(-0.3)-> B5.1
  knitr::kable(summary(lm(B5.1~Alter, data = df))$coefficients, digits = 4, caption = "lm(B5.1~Alter) | Alter: -0.3")
```
\normalsize

- **True effect: -0.3**
- Confidence Interval = $\overline{X}±t_{α/2,N−1}*S_{\overline{X}}$
- **lower.bound** = `r summary(lm(B5.1~Alter, data = df))$coefficients[2,1]` - `r summary(lm(B5.1~Alter, data = df))$coefficients[2,2]` * 1.96 = **`r round(summary(lm(B5.1~Alter, data = df))$coefficients[2,1] - summary(lm(B5.1~Alter, data = df))$coefficients[2,2] * 1.96 ,3)`**
- **upper.bound** = `r summary(lm(B5.1~Alter, data = df))$coefficients[2,1]` + `r summary(lm(B5.1~Alter, data = df))$coefficients[2,2]` * 1.96 = **`r round(summary(lm(B5.1~Alter, data = df))$coefficients[2,1] + summary(lm(B5.1~Alter, data = df))$coefficients[2,2] * 1.96 ,3)`**

-> True effect is **not covered** by the confidence interval


## Brainstroming

<center> **What's the problem? Why can't we estimate the true (causal) effect?** </center>


## Excursus: Modern Causal Analysis

- Satisfaction of the **Conditional Independence Assumption (CIA)** necessary to estimate true causal effects
- Meet the CIA using an **appropriate set of control variables**
- Choose control variables by a **Directed Acyclic Graph (DAG)**


## Excursus MCA: DAG (B5.1 <- FK)
\scriptsize
```{r DAG1, echo=FALSE, message=FALSE, fig.width = 8, fig.height = 3.5}
if(!require(ggdag)) install.packages("ggdag")
library(ggdag)
library(ggplot2)
dag.1 <- dagify(B5.1 ~ FK,
                B5.1 ~B2.1,
                B2.1 ~FK,
                FK ~ Alter,
                B5.1 ~ Alter,
                     labels = c("B5.1" = "B5.1", 
                                "FK" = "FK",
                                "B2.1"= "B2.1",
                                "Alter" = "Alter"),
                     
                     exposure = "FK",
                     outcome = "B5.1")
    
    # ggdag(dag.1, text = FALSE, use_labels = "label") # DAG
    # ggdag_status(dag.1) # DAG with variable status (exposure/outcome/latent)
    # ggdag_paths(dag.1, text = FALSE, use_labels = "label") #exposure and outcome must be defined
    # ggdag_adjustment_set(dag.1, text = FALSE, use_labels = "label") #sets of covariates needed for unbiased estimation
 ggdag_dconnected(dag.1, text = FALSE, use_labels = "label") +
  theme_void() +
  theme(
    plot.background = element_rect(fill = "white"), #darkgrey
    plot.title = element_text(color = "black"),
    plot.subtitle = element_text(color = "black")
  ) + labs(title = "Directed Acyclic Graph", subtitle = "Effect of Interest: B5.1 <- FK")
 
 # FK -(+1.5)-> B5.1
  knitr::kable(summary(lm(B5.1~FK+B2.1+Alter, data = df))$coefficients[1:2,], digits = 4, caption = "lm(B5.1~FK+B2.1+Alter) | FK: +1.5")
  
  #Beta without CIA satisfaction
  print(paste0("Effect without fulfilling the CIA: ", round(summary(lm(B5.1~FK, data = df))$coefficients[2,1], 4)))
```
\normalsize



## Excursus MCA: DAG (B5.1 <- B2.1)
\scriptsize
```{r DAG2, echo=FALSE, message=FALSE, fig.width = 8, fig.height = 3.5}
if(!require(ggdag)) install.packages("ggdag")
library(ggdag)
library(ggplot2)
dag.2 <- dagify(B5.1 ~ FK,
                B5.1 ~B2.1,
                B2.1 ~FK,
                FK ~ Alter,
                B5.1 ~ Alter,
                     labels = c("B5.1" = "B5.1", 
                                "FK" = "FK",
                                "B2.1"= "B2.1",
                                "Alter" = "Alter"),
                     
                     exposure = "B2.1",
                     outcome = "B5.1")
    
    # ggdag(dag.1, text = FALSE, use_labels = "label") # DAG
    # ggdag_status(dag.1) # DAG with variable status (exposure/outcome/latent)
    # ggdag_paths(dag.1, text = FALSE, use_labels = "label") #exposure and outcome must be defined
    # ggdag_adjustment_set(dag.1, text = FALSE, use_labels = "label") #sets of covariates needed for unbiased estimation
 ggdag_dconnected(dag.2, text = FALSE, use_labels = "label") +
  theme_void() +
  theme(
    plot.background = element_rect(fill = "white"), #darkgrey
    plot.title = element_text(color = "black"),
    plot.subtitle = element_text(color = "black")
  ) + labs(title = "Directed Acyclic Graph", subtitle = "Effect of Interest: B5.1 <- B2.1")
 
 # FK -(+1.5)-> B5.1
  knitr::kable(summary(lm(B5.1~B2.1+FK+Alter, data = df))$coefficients[1:2,], digits = 4, caption = "lm(B5.1~B2.1+FK+Alter) | B2.1: +0.2")
  
  #Beta without CIA satisfaction
  print(paste0("Effect without fulfilling the CIA: ", round(summary(lm(B5.1~B2.1, data = df))$coefficients[2,1], 4)))

```
\normalsize


## Excursus MCA: DAG (B5.1 <- Alter)
\scriptsize
```{r DAG3, echo=FALSE, message=FALSE, fig.width = 8, fig.height = 3.5}
if(!require(ggdag)) install.packages("ggdag")
library(ggdag)
library(ggplot2)
dag.3 <- dagify(B5.1 ~ FK,
                B5.1 ~B2.1,
                B2.1 ~FK,
                FK ~ Alter,
                B5.1 ~ Alter,
                     labels = c("B5.1" = "B5.1", 
                                "FK" = "FK",
                                "B2.1"= "B2.1",
                                "Alter" = "Alter"),
                     
                     exposure = "Alter",
                     outcome = "B5.1")
    
    # ggdag(dag.1, text = FALSE, use_labels = "label") # DAG
    # ggdag_status(dag.1) # DAG with variable status (exposure/outcome/latent)
    # ggdag_paths(dag.1, text = FALSE, use_labels = "label") #exposure and outcome must be defined
    # ggdag_adjustment_set(dag.1, text = FALSE, use_labels = "label") #sets of covariates needed for unbiased estimation
 ggdag_dconnected(dag.3, text = FALSE, use_labels = "label") +
  theme_void() +
  theme(
    plot.background = element_rect(fill = "white"), #darkgrey
    plot.title = element_text(color = "black"),
    plot.subtitle = element_text(color = "black")
  ) + labs(title = "Directed Acyclic Graph", subtitle = "Effect of Interest: B5.1 <- Alter")
 
# Alter -(-0.3)-> B5.1
  knitr::kable(summary(lm(B5.1~Alter+FK+B2.1, data = df))$coefficients[1:2,], digits = 4, caption = "lm(B5.1~Alter+FK+B2.1) | Alter: -0.3")
  
    #Beta without CIA satisfaction
  print(paste0("Effect without fulfilling the CIA: ", round(summary(lm(B5.1~Alter, data = df))$coefficients[2,1], 4)))
```
\normalsize


## Conclusion on the calculation of the unbiased effect

- Even in case of non-missing data the estimation of unbiased estimates is not a sure-fire success 
- The CIA needs to be satisfied otherwise there will be **always a bias** in the estimates
- Fulfilling the CIA is possible by deriving an appropriate set of variables from an **DAGs**


# Missing Patterns

## Three types of missing patterns

- Missing completely at random (MCAR)
- Missing Patterns: Missing at random (MAR)
- Missing Patterns: Missing not at random (MNAR)


## Missing Patterns: Missing completely at random (MCAR)

- Missing values are **completely randomly distributed** in the variable of interest
  - There is no pattern to the actual values of the missing variable themselves
    - *e.g. higher or lower values are more likely be missing*
  - Missing data values do not relate to any distribution of another variable
    - *e.g. woman are more likely to have missing values on income*
  
- For instance, when smoking status is not recorded in a random subset of patients
- This missing-type is easy to handle, but unfortunately, data are **almost never** missing completely at random

## MCAR: Inserting Missing values in data frame
\scriptsize
```{r MCAR Inserting Missing values in data frame, echo=TRUE, message=FALSE, fig.width = 8, fig.height = 3.5}
if(!require(missMethods)) install.packages("missMethods")
library(missMethods)

set.seed(322)

df.MCAR = delete_MCAR(df, 0.3, "Mann")
  
df.MCAR = delete_MCAR(df.MCAR, 0.2, "Alter")
    
df.MCAR = delete_MCAR(df.MCAR, 0.25, "FK")
  
df.MCAR = delete_MCAR(df.MCAR, 0.15, "B2.1")
  
df.MCAR = delete_MCAR(df.MCAR, 0.35, "B5.1")
```
\normalsize


## MCAR: Check missing pattern (Mann)
\scriptsize
```{r MCAR check missing pattern (Mann), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  paste0("Mean 'Mann' (MCAR): ", round(mean(df.MCAR$Mann, na.rm = TRUE),4))
  
table(df$Mann, is.na(df.MCAR$Mann))

colPercents(table(df$Mann, is.na(df.MCAR$Mann)))
```
\normalsize


## MCAR: Check missing pattern (Alter)
\scriptsize
```{r MCAR check missing pattern (Alter), echo=FALSE, message=FALSE, warning=FALSE}

  paste0("Mean 'Alter' (no missings): ", mean(df$Alter))
  paste0("Mean 'Alter' (MCAR): ", round(mean(df.MCAR$Alter, na.rm = TRUE),4))
  
table(df$Alter, is.na(df.MCAR$Alter))

colPercents(table(df$Alter, is.na(df.MCAR$Alter)))
```
\normalsize


## MCAR: Check missing pattern (FK)
\scriptsize
```{r MCAR check missing pattern (FK), echo=FALSE, message=FALSE, warning=FALSE}

  paste0("Mean 'FK' (no missings): ", mean(df$FK))
  paste0("Mean 'FK' (MCAR): ", round(mean(df.MCAR$FK, na.rm = TRUE),4))

table(df$FK, is.na(df.MCAR$FK))

colPercents(table(df$FK, is.na(df.MCAR$FK)))
```
\normalsize


## MCAR: Check missing pattern (FK II)
\scriptsize
```{r MCAR check missing pattern (FK II), echo=TRUE, message=FALSE, warning=FALSE}
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    mean(df.MCAR$FK, na.rm = TRUE) + 1.96*std.error(df.MCAR$FK) # upper.bound
    mean(df.MCAR$FK, na.rm = TRUE) - 1.96*std.error(df.MCAR$FK) # lower.bound
```


## MCAR: Check missing pattern (B2.1)
\scriptsize
```{r MCAR check missing pattern (B2.1), echo=FALSE, message=FALSE, warning=FALSE}

  paste0("Mean 'B2.1' (no missings): ", mean(B2.1))
  paste0("Mean 'B2.1' (MCAR): ", round(mean(df.MCAR$B2.1, na.rm = TRUE),4))
  

table(df$B2.1, is.na(df.MCAR$B2.1))

colPercents(table(df$B2.1, is.na(df.MCAR$B2.1)))
```
\normalsize


## MCAR: Check missing pattern (B5.1)
\scriptsize
```{r MCAR check missing pattern (B5.1), echo=FALSE, message=FALSE, warning=FALSE}

  paste0("Mean 'B5.1' (no missings): ", mean(B5.1))
  paste0("Mean 'B5.1' (MCAR): ", round(mean(df.MCAR$B5.1, na.rm = TRUE),4))
  

table(df$B5.1, is.na(df.MCAR$B5.1))

colPercents(table(df$B5.1, is.na(df.MCAR$B5.1)))
```
\normalsize


## Missing Patterns: Missing at random (MAR)

- Confusing and would be better stated as *missing conditionally at random*
- Missing data do have a relationship with other variables in the dataset
  - Whether a value is missing or not depends on other variables
  - *e.g. woman are more likely to have missing values on income*
- The actual shaping of the missing-values are random
  - *e.g. the missing income data of woman are not especially low or high, but even distributed*
- For example, smoking status is not documented in female patients because the doctor was too shy to ask


## MAR: Inserting Missing values in data frame
\scriptsize
```{r MAR Inserting Missing values in data frame, echo=TRUE, message=FALSE, fig.width = 8, fig.height = 3.5}
if(!require(missMethods)) install.packages("missMethods")
library(missMethods)


# Generate MAR values using a censoring mechanism. This leads to a missing value in “X”, if the y-value is below the 30 % quantile of “Y”:
  # ds_mar <- delete_MAR_censoring(ds_comp, 0.3, "X", cols_ctrl = "Y")

#The censoring mechanism is a rather strong form of MAR. A function that allows to control the strength of the MAR mechanism is delete_MAR_1_to_x. The strength is controlled through the argument x: the bigger x, the stronger the simulated MAR mechanism:
  # ds_mar <- delete_MAR_1_to_x(ds_comp, 0.3, "X", cols_ctrl = "Y", x = 2)

set.seed(322)

df.MAR = delete_MAR_1_to_x(df, 0.3, "Mann", cols_ctrl = "Alter", x = 2)
  
df.MAR = delete_MAR_1_to_x(df.MAR, 0.4, "Alter", cols_ctrl = "FK", x = 3)
    
df.MAR = delete_MAR_1_to_x(df.MAR, 0.7, "FK", cols_ctrl = "B2.1", x = 2.5)
  
df.MAR = delete_MAR_1_to_x(df.MAR, 0.15, "B2.1", cols_ctrl = "B5.1", x = 2)
  
df.MAR = delete_MCAR(df.MAR, 0.30, "B5.1")
```
\normalsize


## MAR: Check missing pattern (Mann)
\scriptsize
```{r MAR check missing pattern (Mann), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  paste0("Mean 'Mann' (MAR): ", round(mean(df.MAR$Mann, na.rm = TRUE),4))
  
table(df$Mann, is.na(df.MAR$Mann))

colPercents(table(df$Mann, is.na(df.MAR$Mann)))
```
\normalsize


## MAR: Check missing pattern (Alter)
\scriptsize
```{r MAR check missing pattern (Alter), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'Alter' (no missings): ", mean(df$Alter))
  paste0("Mean 'Alter' (MAR): ", round(mean(df.MAR$Alter, na.rm = TRUE),4))
  
table(df$Alter, is.na(df.MAR$Alter))

colPercents(table(df$Alter, is.na(df.MAR$Alter)))
```
\normalsize


## MAR: Check missing pattern (FK)
\scriptsize
```{r MAR check missing pattern (FK), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'FK' (no missings): ", mean(df$FK))
  paste0("Mean 'FK' (MAR): ", round(mean(df.MAR$FK, na.rm = TRUE),4))
  
table(df$FK, is.na(df.MAR$FK))

colPercents(table(df$FK, is.na(df.MAR$FK)))
```
\normalsize


## MAR: Check missing pattern (B2.1)
\scriptsize
```{r MAR check missing pattern (B2.1), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'B2.1' (no missings): ", mean(df$B2.1))
  paste0("Mean 'B2.1' (MAR): ", round(mean(df.MAR$B2.1, na.rm = TRUE),4))
  
table(df$FK, is.na(df.MAR$FK))

colPercents(table(df$B2.1, is.na(df.MAR$B2.1)))
```
\normalsize


## MAR: Check missing pattern (B5.1)
- Due to estimation reasons B5.1 is MCAR
- Missing patterns is the as shown before






## Missing Patterns: Missing not at random (MNAR)

- The pattern of missingness is related to other variables in the dataset
- In addition, the shaping of the missing-values are **NOT** random
  - Whether a value is missing or not depends on its shaping
  - *e.g. higher or lower incomes are more likely be missing* 
- For example, when smoking status is not recorded in patients admitted as an emergency, who are also more likely to have worse outcomes from surgery

## MNAR: Inserting Missing values in data frame
\scriptsize
```{r MNAR Inserting Missing values in data frame, echo=TRUE, message=FALSE, fig.width = 8, fig.height = 3.5}
if(!require(missMethods)) install.packages("missMethods")
library(missMethods)


# Generate MNAR values using a censoring mechanism. This leads to a missing value in “X”, if the x-value is below the 30 % quantile of “X”:
  #ds_mnar <- delete_MNAR_censoring(ds_comp, 0.3, "X")

# Create missing not at random (MNAR) values using MNAR1:x in a data frame
  #delete_MNAR_1_to_x(ds, 0.2, "X", x = 3)

set.seed(322)

df.MNAR = delete_MNAR_1_to_x(df, 0.3, "Mann", x = 3)
  
df.MNAR = delete_MNAR_1_to_x(df.MNAR, 0.4, "Alter", x = 3.5)
    
df.MNAR = delete_MNAR_1_to_x(df.MNAR, 0.45, "FK", x = 2)
  
df.MNAR = delete_MNAR_1_to_x(df.MNAR, 0.35, "B2.1", x = 2.5)
  
df.MNAR = delete_MNAR_1_to_x(df.MNAR, 0.30, "B5.1", x = 4)
```
\normalsize


## MNAR: Check missing pattern (Mann)
\scriptsize
```{r MNAR check missing pattern (Mann), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  paste0("Mean 'Mann' (MNAR): ", round(mean(df.MNAR$Mann, na.rm = TRUE),4))
  
table(df$Mann, is.na(df.MNAR$Mann))

colPercents(table(df$Mann, is.na(df.MNAR$Mann)))
```
\normalsize


## MNAR: Check missing pattern (Alter)
\scriptsize
```{r MNAR check missing pattern (Alter), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'Alter' (no missings): ", mean(df$Alter))
  paste0("Mean 'Alter' (MNAR): ", round(mean(df.MNAR$Alter, na.rm = TRUE),4))
  
table(df$Alter, is.na(df.MNAR$Alter))

colPercents(table(df$Alter, is.na(df.MNAR$Alter)))
```
\normalsize


## MNAR: Check missing pattern (FK)
\scriptsize
```{r MNAR check missing pattern (FK), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'FK' (no missings): ", mean(df$FK))
  paste0("Mean 'FK' (MNAR): ", round(mean(df.MNAR$FK, na.rm = TRUE),4))
  
table(df$FK, is.na(df.MNAR$FK))

colPercents(table(df$FK, is.na(df.MNAR$FK)))
```
\normalsize


## MNAR: Check missing pattern (B2.1)
\scriptsize
```{r MNAR check missing pattern (B2.1), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'B2.1' (no missings): ", mean(df$B2.1))
  paste0("Mean 'B2.1' (MNAR): ", round(mean(df.MNAR$B2.1, na.rm = TRUE),4))
  
table(df$B2.1, is.na(df.MNAR$B2.1))

colPercents(table(df$B2.1, is.na(df.MNAR$B2.1)))
```
\normalsize


## MNAR: Check missing pattern (B5.1)
\scriptsize
```{r MNAR check missing pattern (B5.1), echo=FALSE, message=FALSE, warning=FALSE}
if(!require(Rcmdr)) install.packages("Rcmdr")
library(Rcmdr)

  paste0("Mean 'B5.1' (no missings): ", mean(df$B5.1))
  paste0("Mean 'B5.1' (MNAR): ", round(mean(df.MNAR$B5.1, na.rm = TRUE),4))
  
table(df$B5.1, is.na(df.MNAR$B5.1))

colPercents(table(df$B5.1, is.na(df.MNAR$B5.1)))
```
\normalsize



# Missing data handling techniques

## Overview of missing data handling techniques

**Traditional Methods**

- Delete Rows with Missing Values (list-wise deletion)
- Mean/Median Imputation
- Regression Imputation

**Modern Methods**

- Multiple Imputation

\scriptsize
| Missingness | List-wise deletion | Mean/Median Imputation | Regression Imputation | Multiple Imputation |
| --- | --- | --- | --- | --- |
| MCAR | unbiased estimate \\ (biases SE) | unbiased estimate \\ (biases SE) | unbiased estimate \\ (biases SE) | unbiased estimate \\ (**unbiased** SE) |
| MAR | biased estimate \\ (biases SE) | biased estimate \\ (biases SE) | unbiased estimate \\ (biases SE) | unbiased estimate \\ (**unbiased** SE) |
| MNAR | biased estimate \\ (biases SE) | biased estimate \\ (biases SE) | unbiased estimate \\ (biases SE) | unbiased estimate \\ (**unbiased** SE) |
\normalsize


## MCAR: list-wise deletion (Point estimate Mann) CODE
\scriptsize
```{r list-wise deletion (Mann) code, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Point estimates
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  paste0("Mean 'Mann' (MCAR): ", round(mean(df.MCAR$Mann, na.rm = TRUE),4))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df.MCAR$Mann, na.rm = TRUE) + 1.96*
                                    std.error(df.MCAR$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df.MCAR$Mann, na.rm = TRUE) - 1.96*
                                    std.error(df.MCAR$Mann), 4)) # lower.bound
```
\normalsize


## MCAR: list-wise deletion (Point estimate Mann)
**Full Dataset (true coefficients)**
\scriptsize
```{r MCAR list-wise deletion (Mann) full, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Point estimates
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df$Mann, na.rm = TRUE) + 1.96*std.error(df$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df$Mann, na.rm = TRUE) - 1.96*std.error(df$Mann), 4)) # lower.bound
```
\normalsize

**Missing Dataset **
\scriptsize
```{r MCAR list-wise deletion (Mann) miss, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Point estimates
  paste0("Mean 'Mann' (MCAR): ", round(mean(df.MCAR$Mann, na.rm = TRUE),4))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df.MCAR$Mann, na.rm = TRUE) + 1.96*std.error(df.MCAR$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df.MCAR$Mann, na.rm = TRUE) - 1.96*std.error(df.MCAR$Mann), 4)) # lower.bound
```
\normalsize

**Evaluation**: Unbiased Estimate (**biases** SE \& Confidence Interval)



## MCAR: Mean/Median Imputation (Point estimate Mann) CODE
\scriptsize
```{r Mean/Median Imputation (Mann) code, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# Mean Imputation
  df.MCAR.Mean = df.MCAR
  df.MCAR.Mean$Mann <- ifelse(is.na(df.MCAR$Mann), mean(df.MCAR$Mann, na.rm = TRUE), 
                              df.MCAR$Mann)

# Point estimates
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  paste0("Mean 'Mann' (MCAR): ", round(mean(df.MCAR.Mean$Mann, na.rm = TRUE),4))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df.MCAR.Mean$Mann, na.rm = TRUE) + 1.96*
                                    std.error(df.MCAR.Mean$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df.MCAR.Mean$Mann, na.rm = TRUE) - 1.96*
                                    std.error(df.MCAR.Mean$Mann), 4)) # lower.bound
```
\normalsize


## MCAR: Mean/Median Imputation (Point estimate Mann)
**Full Dataset (true coefficients)**
\scriptsize
```{r MCAR Mean/Median Imputation (Mann) full, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Point estimate
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df$Mann, na.rm = TRUE) + 1.96*std.error(df$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df$Mann, na.rm = TRUE) - 1.96*std.error(df$Mann), 4)) # lower.bound
```
\normalsize

**Missing Dataset **
\scriptsize
```{r MCAR Mean/Median Imputation (Mann) miss, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Mean Imputation
  df.MCAR.Mean = df.MCAR
  df.MCAR.Mean$Mann <- ifelse(is.na(df.MCAR$Mann), mean(df.MCAR$Mann, na.rm = TRUE), df.MCAR$Mann)

# Point estimates
  paste0("Mean 'Mann' (MCAR): ", round(mean(df.MCAR.Mean$Mann, na.rm = TRUE),4))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)]))
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df.MCAR.Mean$Mann, na.rm = TRUE) + 1.96*std.error(df.MCAR.Mean$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df.MCAR.Mean$Mann, na.rm = TRUE) - 1.96*std.error(df.MCAR.Mean$Mann), 4)) # lower.bound
```
\normalsize

**Evaluation**: Unbiased Estimate (**biases** SE \& Confidence Interval)



## Excursus: Rules for MI inference ("Rubin's rules")

[*Literature-Link*](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2727536/)

**Combining parameter estimates**

For a single population parameter of interest, Q, e.g. a regression coefficient, the MI overall point estimate is the average of the **m**-estimates of Q from the imputed datasets:
$\overline{Q}=\frac{1}{m} \sum_{i=1}^{m} \hat{Q}$

The **standard error** is calculated with the **total variance**(TIV), which is uses the **within imputation variance**(WIV) and the **between imputation variance**(BIV):

$WIV = \frac{1}{m} \sum_{i=1}^{m} σ^{2}$, where $σ^{2}=\frac{\sum(X_i-μ)^{2}}{N}$

$BIV = \frac{1}{m} \sum_{i=1}^{m}(\hat{Q}_i - \overline{Q})$

$TIV = WIV + (1+ \frac{1}{m})BIW$

Now the standard error is calculated by $SD = \sqrt{TIV}$ and $SE = \frac{SD}{\sqrt{N/m}}$


## Testing "Rubin's rules" code
\tiny
```{r Testing Rubins rules code, echo=TRUE, eval=F, message=FALSE, warning=FALSE}
#Create artificial mi-dataset (no missings)
  df.mi = rbind(df,df,df,df,df) %>% mutate(mi_id = rep(1:5, each = 5000))
  n_mi = length(unique(df.mi$mi_id))
#Calculate and compare point-estimates
  round(mean(df$Mann),4)
  round(mean(df.mi$Mann),4)
#Calculate and compare standard errors & confidence intervals
  #1.Step: Calculate the total variance by the "within imputation variance" and the "between imputation variance"
  #2.Step: Calculate the SE from the "total variance"
    WIV = df.mi %>% group_by(mi_id) %>% summarise(var = var(Mann)) %>% summarise(var = mean(var)) %>% .[[1]]
    
    BIV = df.mi %>% group_by(mi_id) %>% summarise(mean = mean(Mann)) %>% mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
      summarise(B = mean(diff.square)) %>% .[[1]]
    
    TIV = WIV+(1+1/5)*BIV
    SD = sqrt(TIV)
    SE = SD/sqrt(nrow(df.mi)/5)
      
  # Alternative: directly use SE as within-component
    W = df.mi %>% group_by(mi_id) %>% summarise(std.error = sd(Mann[!is.na(Mann)])/sqrt(length(Mann[!is.na(Mann)]))) %>%
      summarise(std.error = mean(std.error)) %>%
      .[[1]]
    
    B = df.mi %>% group_by(mi_id) %>% summarise(mean = mean(Mann)) %>% mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
      summarise(B = mean(diff.square)) %>% .[[1]]
    SE.short = W+(1+1/n_mi)*B

    
  #Alternative: Neglect between-component
    #define standard error of mean function
      mi_std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])/n_mi) # UNbiased!!!!
  
  #Define standard error without considering MI-structure (WRONG)
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) #Biased!!!!
```
\normalsize

## Testing "Rubin's rules" code
**Mean Calculation:**
```{r Testing Rubins rules, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#Create artificial mi-dataset (no missings)
  df.mi = rbind(df,df,df,df,df) %>% mutate(mi_id = rep(1:5, each = 5000))
  n_mi = length(unique(df.mi$mi_id))

#Calculate and compare point-estimates
  paste0("Mean (Full-Dataset): ", round(mean(df$Mann),4))
  paste0("Mean (MI-Dataset): ", round(mean(df.mi$Mann),4))
```

**SE Calculation:**
```{r Testing Rubins rules 2, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
#Calculate and compare standard errors & confidence intervals

  #Literature: 
    # - 2004 - Rubin DB. Multiple Imputation for Nonresponse in Surveys. New York: John Wiley and Sons; 
    # - 2009.BMC Med Res Methodl - Marshall et al - Combining estimates of interest in prognostic modelling studies after multiple imputation: current practice and guidelines
  
  #1.Step: Calculate the total variance by the "within imputation variance" and the "between imputation variance"
  #2.Step: Calculate the SE from the "total variance"
    WIV = df.mi %>%
      group_by(mi_id) %>%
      summarise(var = var(Mann)) %>%
      summarise(var = mean(var)) %>%
      .[[1]]
    
    BIV = df.mi %>%
      group_by(mi_id) %>%
      summarise(mean = mean(Mann)) %>%
      mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
      summarise(B = mean(diff.square)) %>%
      .[[1]]
    
    TIV = WIV+(1+1/5)*BIV
    
    SD = sqrt(TIV)
    SE = SD/sqrt(nrow(df.mi)/5)
    
    
    #Define function for se-calculation
      mi_se = function(data, var, mi_id) {
        WIV = data %>%
          #group_by(eval(parse(text =paste0("`",mi_id,"`")))) %>%
          group_by(eval(parse(text =mi_id))) %>%
          summarise(variance = var(eval(parse(text=var)))) %>%
          summarise(var = mean(variance)) %>%
          .[[1]]
        
        BIV = data %>%
          #group_by(eval(parse(text =paste0("`",mi_id,"`")))) %>%
          group_by(eval(parse(text =mi_id))) %>%
          summarise(mean = mean(eval(parse(text=var)))) %>%
          mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
          summarise(B = mean(diff.square)) %>%
          .[[1]]
        
        TIV = WIV+(1+1/length(unique(data[,mi_id])))*BIV
        
        SD = sqrt(TIV)
        SE = SD/sqrt(nrow(data)/length(unique(data[,mi_id])))
        return(SE)
      }
  
  # Alternative: directly use SE as within-component
    W = df.mi %>%
      group_by(mi_id) %>%
      summarise(std.error = sd(Mann[!is.na(Mann)])/sqrt(length(Mann[!is.na(Mann)]))) %>%
      summarise(std.error = mean(std.error)) %>%
      .[[1]]
    
    B = df.mi %>%
      group_by(mi_id) %>%
      summarise(mean = mean(Mann)) %>%
      mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
      summarise(B = mean(diff.square)) %>%
      .[[1]]
    
    SE.short = W+(1+1/n_mi)*B

  #Alternative: Neglect between-component
    #define standard error of mean function
      mi_std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])/n_mi) # UNbiased!!!!
  
  #Comparison SEs
    #define standard error of mean function (WRONG)
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) #Biased!!!!
  
    paste0("SE (Not considering MI-structure): ", std.error(df.mi$Mann))
    paste0("SE (No between effect): ", mi_std.error(df.mi$Mann))
    paste0("SE (Rubin's rule, SE indead of VAR): ", SE.short)
    paste0("SE (Rubin's rule): ", mi_se(df.mi,"Mann"))
  
  #Comparison CIs
#    round(mean(df.mi$Mann) - 1.96* std.error(df.mi$Mann), 4)
#    round(mean(df.mi$Mann) + 1.96* std.error(df.mi$Mann), 4)
    
#    round(mean(df.mi$Mann) - 1.96* mi_std.error(df.mi$Mann), 4)
#    round(mean(df.mi$Mann) + 1.96* mi_std.error(df.mi$Mann), 4)
    
#    round(mean(df.mi$Mann) - 1.96* SE, 4)
#    round(mean(df.mi$Mann) + 1.96* SE, 4)
    
#    round(mean(df.mi$Mann) - 1.96* mi_se(df.mi,"Mann"), 4)
#    round(mean(df.mi$Mann) + 1.96* mi_se(df.mi,"Mann"), 4)
  
#    df.mi.list = list(df,df,df,df,df)
#    with(df.mi.list, t.test(Mann, conf.level = 0.95))
```


## MCAR: Multiple Imputation (Point estimate Mann) CODE
\scriptsize
\tiny
```{r Multiple Imputation (Mann) code, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
if(!require(mice)) install.packages("mice")
library(mice)

# Multiple Imputation
  df.MCAR.MI <- mice(df.MCAR,m=5,maxit=50,meth='pmm',seed=500, print=FALSE) # Object of mice class, can be used with mice-specific command
  df.MCAR.MI <- complete(df.MCAR.MI, 'long') # create dataframe of the n-imputed datasets
  
# Point estimates
  paste0("Mean 'Mann' (MCAR): ", round(mean(df.MCAR.MI$Mann),4))
  
#Define function for SE-calculation
  mi_se = function(data, var, mi_id) {
    WIV = data %>%
      group_by(eval(parse(text =mi_id))) %>%
      summarise(variance = var(eval(parse(text=var)))) %>%
      summarise(var = mean(variance)) %>%
      .[[1]]
    BIV = data %>%
      group_by(eval(parse(text =mi_id))) %>%
      summarise(mean = mean(eval(parse(text=var)))) %>%
      mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
      summarise(B = mean(diff.square)) %>%
      .[[1]]
    TIV = WIV+(1+1/length(unique(data[,mi_id])))*BIV
    SD = sqrt(TIV)
    SE = SD/sqrt(nrow(data)/length(unique(data[,mi_id])))
    return(SE)
  }
  
#Using the function
  mi_se(df.mi,"Mann", "mi_id")
```
\normalsize


## MCAR: Multiple Imputation (Point estimate Mann) 
**Full Dataset (true coefficients)**
\scriptsize
```{r MCAR Multiple Imputation (Mann) full, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Point estimate
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) 
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df$Mann, na.rm = TRUE) + 1.96*std.error(df$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df$Mann, na.rm = TRUE) - 1.96*std.error(df$Mann), 4)) # lower.bound
```
\normalsize

**Missing Dataset **
\scriptsize
```{r MCAR Multiple Imputation (Mann) miss wrong, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
if(!require(mice)) install.packages("mice")
library(mice)

# Multiple Imputation
  df.MCAR.MI <- mice(df.MCAR,m=5,maxit=50,meth='pmm',seed=500, print=FALSE) # Object of mice class, can be used with mice-specific command
  df.MCAR.MI <- complete(df.MCAR.MI, 'long') # create dataframe of the n-imputed datasets

# Point estimates
  mean.df.MCAR.MI = df.MCAR.MI %>% summarise(mean = mean(Mann)) %>% summarise(mean = mean(mean)) %>% .[[1]]
  paste0("Mean 'Mann' (MCAR): ", round(mean.df.MCAR.MI,4))
  
  #define standard error of mean function (WRONG)
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) #Biased!!!!
  
  #Calculate the confidence interval (wrong way)
    paste0("upper.bound (wrong): ", round(mean(df.MCAR.MI$Mann) + 1.96* std.error(df.MCAR.MI$Mann), 4)) # upper.bound
    paste0("lower.bound (wrong): ", round(mean(df.MCAR.MI$Mann) - 1.96* std.error(df.MCAR.MI$Mann), 4)) # lower.bound
  
  #Calculate the confidence interval (correct)
    #Define function for se-calculation
      mi_se = function(data, var, mi_id) {
        WIV = data %>%
          group_by(eval(parse(text=mi_id))) %>%
          summarise(variance = var(eval(parse(text=var)))) %>%
          summarise(var = mean(variance)) %>%
          .[[1]]
        
        BIV = data %>%
          group_by(eval(parse(text=mi_id))) %>%
          summarise(mean = mean(eval(parse(text=var)))) %>%
          mutate(diff.square = (mean - colMeans(.[2]))^2) %>%
          summarise(B = mean(diff.square)) %>%
          .[[1]]
        
        TIV = WIV+(1+1/length(unique(data[,mi_id])))*BIV
        
        SD = sqrt(TIV)
        SE = SD/sqrt(nrow(data)/length(unique(data[,mi_id])))
        return(SE)
      }
        
      
    paste0("upper.bound (true): ", round(mean(df.MCAR.MI$Mann) + 1.96* mi_se(df.MCAR.MI,"Mann", ".imp"), 4)) # upper.bound
    paste0("lower.bound (true): ", round(mean(df.MCAR.MI$Mann) - 1.96* mi_se(df.MCAR.MI,"Mann", ".imp"), 4)) # lower.bound
```
\normalsize

**Evaluation**: Unbiased Estimate (unbiases SE \& Confidence Interval)







## MAR: Multiple Imputation (Point estimate Mann) 
**Full Dataset (true coefficients)**
\scriptsize
```{r MAR Multiple Imputation (Mann) full, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Point estimate
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) 
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df$Mann, na.rm = TRUE) + 1.96*std.error(df$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df$Mann, na.rm = TRUE) - 1.96*std.error(df$Mann), 4)) # lower.bound
```
\normalsize

**Missing Dataset **
\scriptsize
```{r MAR Multiple Imputation (Mann) miss wrong, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
if(!require(mice)) install.packages("mice")
library(mice)

# Multiple Imputation
  df.MAR.MI <- mice(df.MAR,m=5,maxit=50,meth='pmm',seed=500, print=FALSE) # Object of mice class, can be used with mice-specific command
  df.MAR.MI <- complete(df.MAR.MI, 'long') # create dataframe of the n-imputed datasets

# Point estimates
  mean.df.MAR.MI = df.MAR.MI %>% summarise(mean = mean(Mann)) %>% summarise(mean = mean(mean)) %>% .[[1]]
  paste0("Mean 'Mann' (MAR): ", round(mean.df.MAR.MI,4))
  
  #define standard error of mean function (WRONG)
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) #Biased!!!!
  
  #Calculate the confidence interval (wrong way)
    paste0("upper.bound (wrong): ", round(mean(df.MAR.MI$Mann) + 1.96* std.error(df.MAR.MI$Mann), 4)) # upper.bound
    paste0("lower.bound (wrong): ", round(mean(df.MAR.MI$Mann) - 1.96* std.error(df.MAR.MI$Mann), 4)) # lower.bound
  

  #Calculate the confidence interval (correct)
     #define standard error for MI datasets
      # [see above]
    paste0("upper.bound (true): ", round(mean(df.MAR.MI$Mann) + 1.96* mi_se(df.MAR.MI,"Mann", ".imp"), 4)) # upper.bound
    paste0("lower.bound (true): ", round(mean(df.MAR.MI$Mann) - 1.96* mi_se(df.MAR.MI,"Mann", ".imp"), 4)) # lower.bound

    
    

```
\normalsize

**Evaluation**: Unbiased Estimate (**unbiases SE** \& Confidence Interval)




## MNAR: Multiple Imputation (Point estimate Mann) 
**Full Dataset (true coefficients)**
\scriptsize
```{r MNAR Multiple Imputation (Mann) full, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Point estimate
  paste0("Mean 'Mann' (no missings): ", mean(df$Mann))
  
  #define standard error of mean function
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) 
  
  #Calculate the confidence interval
    paste0("upper.bound: ", round(mean(df$Mann, na.rm = TRUE) + 1.96*std.error(df$Mann), 4)) # upper.bound
    paste0("lower.bound: ", round(mean(df$Mann, na.rm = TRUE) - 1.96*std.error(df$Mann), 4)) # lower.bound
```
\normalsize

**Missing Dataset **
\scriptsize
```{r MNAR Multiple Imputation (Mann) miss wrong, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
if(!require(mice)) install.packages("mice")
library(mice)

# Multiple Imputation
  df.MNAR.MI <- mice(df.MNAR,m=5,maxit=50,meth='pmm',seed=500, print=FALSE) # Object of mice class, can be used with mice-specific command
  df.MNAR.MI <- complete(df.MNAR.MI, 'long') # create dataframe of the n-imputed datasets

# Point estimates
  mean.df.MNAR.MI = df.MNAR.MI %>% summarise(mean = mean(Mann)) %>% summarise(mean = mean(mean)) %>% .[[1]]
  paste0("Mean 'Mann' (MAR): ", round(mean.df.MNAR.MI,4))
  
  #define standard error of mean function (WRONG)
    std.error <- function(x) sd(x[!is.na(x)])/sqrt(length(x[!is.na(x)])) #Biased!!!!
  
  #Calculate the confidence interval (wrong way)
    paste0("upper.bound (wrong): ", round(mean(df.MNAR.MI$Mann) + 1.96* std.error(df.MNAR.MI$Mann), 4)) # upper.bound
    paste0("lower.bound (wrong): ", round(mean(df.MNAR.MI$Mann) - 1.96* std.error(df.MNAR.MI$Mann), 4)) # lower.bound
  

  #Calculate the confidence interval (correct)
     #define standard error for MI datasets
      # [see above]
    paste0("upper.bound (true): ", round(mean(df.MNAR.MI$Mann) + 1.96* mi_se(df.MNAR.MI,"Mann", ".imp"), 4)) # upper.bound
    paste0("lower.bound (true): ", round(mean(df.MNAR.MI$Mann) - 1.96* mi_se(df.MNAR.MI,"Mann", ".imp"), 4)) # lower.bound

    
```
\normalsize

**Evaluation**: Unbiased Estimate (**unbiases SE** \& Confidence Interval)




# Questions

<center> **Any Questions so far?** </center>

